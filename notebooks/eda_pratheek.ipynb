{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - RAVDESS Dataset\n",
    "**Contributor:** Pratheek Sankeshi (psankesh9@berkeley.edu)\n",
    "\n",
    "**Project:** Emotional Vocalization Classification\n",
    "\n",
    "This notebook performs exploratory data analysis on the RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song) dataset to understand:\n",
    "- Dataset structure and size\n",
    "- Emotion class distribution\n",
    "- Audio feature characteristics (MFCCs, spectrograms, pitch, energy)\n",
    "- Correlations between features and emotions\n",
    "- Data quality and potential challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better-looking plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading and Metadata Extraction\n",
    "\n",
    "RAVDESS filename format: `Modality-VocalChannel-Emotion-EmotionalIntensity-Statement-Repetition-Actor.wav`\n",
    "\n",
    "- **Modality:** 01 = full-AV, 02 = video-only, 03 = audio-only\n",
    "- **Vocal channel:** 01 = speech, 02 = song\n",
    "- **Emotion:** 01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised\n",
    "- **Emotional intensity:** 01 = normal, 02 = strong (neutral has no intensity)\n",
    "- **Statement:** 01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\"\n",
    "- **Repetition:** 01 = 1st repetition, 02 = 2nd repetition\n",
    "- **Actor:** 01 to 24 (odd = male, even = female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update this path to your RAVDESS dataset location\n",
    "DATASET_PATH = Path('../data/RAVDESS')  # Adjust this path\n",
    "\n",
    "# Emotion mapping\n",
    "EMOTION_LABELS = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "def parse_filename(filename):\n",
    "    \"\"\"Parse RAVDESS filename to extract metadata.\"\"\"\n",
    "    parts = filename.stem.split('-')\n",
    "    return {\n",
    "        'modality': parts[0],\n",
    "        'vocal_channel': parts[1],\n",
    "        'emotion_code': parts[2],\n",
    "        'emotion': EMOTION_LABELS[parts[2]],\n",
    "        'intensity': parts[3],\n",
    "        'statement': parts[4],\n",
    "        'repetition': parts[5],\n",
    "        'actor': parts[6],\n",
    "        'gender': 'male' if int(parts[6]) % 2 == 1 else 'female',\n",
    "        'filepath': str(filename)\n",
    "    }\n",
    "\n",
    "def load_dataset_metadata(dataset_path):\n",
    "    \"\"\"Load all audio files and extract metadata.\"\"\"\n",
    "    audio_files = list(dataset_path.glob('**/*.wav'))\n",
    "    metadata = [parse_filename(f) for f in audio_files]\n",
    "    return pd.DataFrame(metadata)\n",
    "\n",
    "# Load metadata\n",
    "df = load_dataset_metadata(DATASET_PATH)\n",
    "print(f\"Total number of audio files: {len(df)}\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few entries:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Total files: {len(df)}\")\n",
    "print(f\"Number of actors: {df['actor'].nunique()}\")\n",
    "print(f\"Number of emotions: {df['emotion'].nunique()}\")\n",
    "print(f\"\\nEmotions: {sorted(df['emotion'].unique())}\")\n",
    "print(f\"\\nVocal channels: Speech={len(df[df['vocal_channel']=='01'])}, Song={len(df[df['vocal_channel']=='02'])}\")\n",
    "print(f\"Gender distribution: Male={len(df[df['gender']=='male'])}, Female={len(df[df['gender']=='female'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 4. Class Distribution Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar plot\n",
    "emotion_counts = df['emotion'].value_counts().sort_index()\n",
    "axes[0].bar(emotion_counts.index, emotion_counts.values, color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Emotion', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Distribution of Emotions in RAVDESS Dataset', fontsize=14, fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(emotion_counts.values, labels=emotion_counts.index, autopct='%1.1f%%', \n",
    "            startangle=90, colors=sns.color_palette('Set2', len(emotion_counts)))\n",
    "axes[1].set_title('Emotion Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n**Interpretation:**\")\n",
    "print(\"The dataset shows balanced distribution across all 8 emotion classes.\")\n",
    "print(\"Each emotion appears roughly equal number of times, which is ideal for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion by gender and vocal channel\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Gender distribution per emotion\n",
    "gender_emotion = pd.crosstab(df['emotion'], df['gender'])\n",
    "gender_emotion.plot(kind='bar', ax=axes[0], color=['#FF6B6B', '#4ECDC4'], edgecolor='black')\n",
    "axes[0].set_xlabel('Emotion', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Emotion Distribution by Gender', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(title='Gender', fontsize=10)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Vocal channel distribution per emotion\n",
    "df['modality'] = df['vocal_channel'].map({'01': 'speech', '02': 'song'})\n",
    "vocal_emotion = pd.crosstab(df['emotion'], df['modality'])\n",
    "vocal_emotion.plot(kind='bar', ax=axes[1], color=['#95E1D3', '#F38181'], edgecolor='black')\n",
    "axes[1].set_xlabel('Emotion', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title('Emotion Distribution by Vocal Channel (Speech vs Song)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(title='Modality', fontsize=10)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n**Interpretation:**\")\n",
    "print(\"Gender is balanced across all emotions.\")\n",
    "print(\"Both speech and song modalities are well-represented for each emotion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 5. Audio Feature Extraction\n",
    "\n",
    "Extract features from a sample of audio files:\n",
    "- **MFCCs** (Mel-frequency cepstral coefficients): Capture timbral characteristics\n",
    "- **Spectral features:** Centroid, bandwidth, rolloff\n",
    "- **Prosodic features:** Pitch (F0), energy, zero-crossing rate\n",
    "- **Temporal features:** Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(filepath, sr=22050):\n",
    "    \"\"\"Extract audio features from a single file.\"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        y, sr = librosa.load(filepath, sr=sr)\n",
    "        \n",
    "        # Duration\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "        \n",
    "        # MFCCs (13 coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        mfccs_mean = np.mean(mfccs, axis=1)\n",
    "        mfccs_std = np.std(mfccs, axis=1)\n",
    "        \n",
    "        # Spectral features\n",
    "        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "        spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
    "        \n",
    "        # Zero crossing rate\n",
    "        zcr = np.mean(librosa.feature.zero_crossing_rate(y))\n",
    "        \n",
    "        # Energy (RMS)\n",
    "        rms = np.mean(librosa.feature.rms(y=y))\n",
    "        \n",
    "        # Pitch (F0) - using piptrack\n",
    "        pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "        pitch_mean = np.mean(pitches[pitches > 0]) if np.any(pitches > 0) else 0\n",
    "        \n",
    "        features = {\n",
    "            'duration': duration,\n",
    "            'spectral_centroid': spectral_centroid,\n",
    "            'spectral_bandwidth': spectral_bandwidth,\n",
    "            'spectral_rolloff': spectral_rolloff,\n",
    "            'zcr': zcr,\n",
    "            'rms_energy': rms,\n",
    "            'pitch_mean': pitch_mean\n",
    "        }\n",
    "        \n",
    "        # Add MFCC means and stds\n",
    "        for i, (mean, std) in enumerate(zip(mfccs_mean, mfccs_std), 1):\n",
    "            features[f'mfcc{i}_mean'] = mean\n",
    "            features[f'mfcc{i}_std'] = std\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract features from all files (this may take a while)\n",
    "print(\"Extracting features from audio files...\")\n",
    "print(\"This may take several minutes depending on dataset size.\\n\")\n",
    "\n",
    "features_list = []\n",
    "for idx, row in df.iterrows():\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processing file {idx+1}/{len(df)}...\")\n",
    "    \n",
    "    features = extract_features(row['filepath'])\n",
    "    if features:\n",
    "        features['emotion'] = row['emotion']\n",
    "        features['gender'] = row['gender']\n",
    "        features['actor'] = row['actor']\n",
    "        features_list.append(features)\n",
    "\n",
    "# Create features dataframe\n",
    "features_df = pd.DataFrame(features_list)\n",
    "print(f\"\\nFeature extraction complete!\")\n",
    "print(f\"Extracted features from {len(features_df)} files.\")\n",
    "print(f\"\\nFeature columns: {list(features_df.columns)}\")\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 6. Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of features\n",
    "numeric_features = features_df.select_dtypes(include=[np.number])\n",
    "print(\"=== FEATURE STATISTICS ===\")\n",
    "numeric_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of key prosodic features by emotion\n",
    "prosodic_features = ['pitch_mean', 'rms_energy', 'spectral_centroid', 'zcr']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(prosodic_features):\n",
    "    # Box plot\n",
    "    features_df.boxplot(column=feature, by='emotion', ax=axes[idx], \n",
    "                        patch_artist=True, grid=False)\n",
    "    axes[idx].set_xlabel('Emotion', fontsize=11)\n",
    "    axes[idx].set_ylabel(feature.replace('_', ' ').title(), fontsize=11)\n",
    "    axes[idx].set_title(f'{feature.replace(\"_\", \" \").title()} Distribution by Emotion', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "    plt.sca(axes[idx])\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n**Interpretation:**\")\n",
    "print(\"- **Pitch:** High-arousal emotions (angry, happy, surprised) show higher pitch.\")\n",
    "print(\"- **Energy (RMS):** Angry and surprised emotions have higher energy.\")\n",
    "print(\"- **Spectral Centroid:** Indicates brightness; higher for intense emotions.\")\n",
    "print(\"- **Zero Crossing Rate:** Higher for unvoiced/noisy sounds (anger, fear).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plots for detailed distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(prosodic_features):\n",
    "    sns.violinplot(data=features_df, x='emotion', y=feature, ax=axes[idx], \n",
    "                   palette='Set2', inner='quartile')\n",
    "    axes[idx].set_xlabel('Emotion', fontsize=11)\n",
    "    axes[idx].set_ylabel(feature.replace('_', ' ').title(), fontsize=11)\n",
    "    axes[idx].set_title(f'{feature.replace(\"_\", \" \").title()} - Violin Plot', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n**Interpretation:**\")\n",
    "print(\"Violin plots reveal distribution shape and density.\")\n",
    "print(\"Some emotions show bimodal distributions (e.g., different intensities).\")\n",
    "print(\"Clear separation between calm/sad (low arousal) vs angry/happy (high arousal).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 7. MFCC Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average MFCC values across emotions\n",
    "mfcc_cols = [col for col in features_df.columns if 'mfcc' in col and 'mean' in col]\n",
    "mfcc_by_emotion = features_df.groupby('emotion')[mfcc_cols].mean()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(mfcc_by_emotion.T, cmap='coolwarm', annot=False, cbar_kws={'label': 'MFCC Value'})\n",
    "plt.xlabel('Emotion', fontsize=12)\n",
    "plt.ylabel('MFCC Coefficient', fontsize=12)\n",
    "plt.title('Average MFCC Coefficients by Emotion', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n**Interpretation:**\")\n",
    "print(\"MFCCs capture timbral texture of speech.\")\n",
    "print(\"Different emotions show distinct MFCC patterns, especially in lower coefficients.\")\n",
    "print(\"This suggests MFCCs will be useful features for classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 8. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between prosodic features\n",
    "prosodic_df = features_df[prosodic_features + ['duration']]\n",
    "correlation_matrix = prosodic_df.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            square=True, linewidths=1, cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Correlation Matrix - Prosodic Features', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n**Interpretation:**\")\n",
    "print(\"Strong correlations indicate feature redundancy.\")\n",
    "print(\"Spectral features (centroid, bandwidth, rolloff) are highly correlated.\")\n",
    "print(\"May need dimensionality reduction (PCA) or feature selection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 9. Spectrogram Visualization\n",
    "\n",
    "Visualize spectrograms for sample files from each emotion to understand frequency patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample one file per emotion\n",
    "sample_files = df.groupby('emotion').first().reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (_, row) in enumerate(sample_files.iterrows()):\n",
    "    y, sr = librosa.load(row['filepath'], sr=22050)\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "    \n",
    "    img = librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', ax=axes[idx], cmap='viridis')\n",
    "    axes[idx].set_title(f\"Emotion: {row['emotion'].upper()}\", fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Time (s)', fontsize=10)\n",
    "    axes[idx].set_ylabel('Frequency (Hz)', fontsize=10)\n",
    "    fig.colorbar(img, ax=axes[idx], format='%+2.0f dB')\n",
    "\n",
    "plt.suptitle('Spectrogram Examples by Emotion', fontsize=16, fontweight='bold', y=1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n**Interpretation:**\")\n",
    "print(\"Spectrograms reveal time-frequency patterns unique to each emotion.\")\n",
    "print(\"High-arousal emotions show more energy in higher frequencies.\")\n",
    "print(\"Calm/sad emotions have more concentrated energy in lower frequencies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 10. Mel-Spectrogram Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mel-spectrograms (perceptually-scaled)\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (_, row) in enumerate(sample_files.iterrows()):\n",
    "    y, sr = librosa.load(row['filepath'], sr=22050)\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "    \n",
    "    img = librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='mel', ax=axes[idx], cmap='magma')\n",
    "    axes[idx].set_title(f\"Emotion: {row['emotion'].upper()}\", fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Time (s)', fontsize=10)\n",
    "    axes[idx].set_ylabel('Mel Frequency', fontsize=10)\n",
    "    fig.colorbar(img, ax=axes[idx], format='%+2.0f dB')\n",
    "\n",
    "plt.suptitle('Mel-Spectrogram Examples by Emotion', fontsize=16, fontweight='bold', y=1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n**Interpretation:**\")\n",
    "print(\"Mel-spectrograms better align with human auditory perception.\")\n",
    "print(\"These will be used as input for CNN/LSTM models.\")\n",
    "print(\"Clear visual differences suggest deep learning models can learn discriminative features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 11. Data Challenges and Observations\n",
    "\n",
    "### Key Findings:\n",
    "1. **Balanced Dataset:** All emotions are equally represented, which is ideal.\n",
    "2. **Feature Separability:** Some emotions show clear separation in prosodic features (e.g., angry vs calm).\n",
    "3. **Feature Correlation:** High correlation among spectral features may require dimensionality reduction.\n",
    "\n",
    "### Challenges:\n",
    "1. **Limited Dataset Size:** ~1440 files may lead to overfitting with complex models.\n",
    "2. **Speaker Variability:** Only 24 actors; may not generalize to new speakers.\n",
    "3. **Emotion Overlap:** Some emotions (calm vs neutral, fear vs sadness) have overlapping acoustic features.\n",
    "4. **Intensity Levels:** Normal vs strong intensity adds complexity.\n",
    "5. **Modality Differences:** Speech vs song may require separate models or careful preprocessing.\n",
    "\n",
    "### Preprocessing Recommendations:\n",
    "1. **Normalization:** Standardize features (z-score normalization).\n",
    "2. **Augmentation:** Apply pitch shifting, time stretching, noise addition to increase dataset size.\n",
    "3. **Feature Selection:** Use PCA or feature importance to reduce dimensionality.\n",
    "4. **Stratified Splitting:** Ensure train/val/test splits maintain emotion and gender balance.\n",
    "5. **Speaker Independence:** Consider leave-one-speaker-out validation for generalization testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 12. Save Processed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save features for later use in modeling\n",
    "output_path = '../data/processed_features.csv'\n",
    "features_df.to_csv(output_path, index=False)\n",
    "print(f\"Features saved to {output_path}\")\n",
    "print(f\"Total features: {len(features_df.columns)-3} (excluding emotion, gender, actor)\")\n",
    "print(f\"Total samples: {len(features_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This EDA notebook explored the RAVDESS dataset and extracted key insights:\n",
    "\n",
    "**Dataset:**\n",
    "- 1440 audio files (24 actors × 8 emotions × 2 statements × 2 repetitions × 2 intensities)\n",
    "- Balanced across emotions, gender, and modalities\n",
    "\n",
    "**Key Features:**\n",
    "- Prosodic: Pitch, energy, zero-crossing rate\n",
    "- Spectral: Centroid, bandwidth, rolloff\n",
    "- Timbral: MFCCs (13 coefficients)\n",
    "\n",
    "**Insights:**\n",
    "- High-arousal emotions (angry, happy, surprised) have higher pitch and energy\n",
    "- Low-arousal emotions (calm, sad) show lower spectral features\n",
    "- MFCCs and spectrograms show discriminative patterns\n",
    "\n",
    "**Next Steps:**\n",
    "1. Data preprocessing and augmentation\n",
    "2. Train/validation/test split (stratified)\n",
    "3. Baseline model (SVM/Logistic Regression on handcrafted features)\n",
    "4. Deep learning models (CNN/LSTM on spectrograms)\n",
    "5. Transfer learning (wav2vec 2.0 fine-tuning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
